{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import json\n",
    "from datetime import datetime\n",
    "import email.utils\n",
    "\n",
    "RSS_FEED_URL = [\n",
    "    'http://www.dn.se/nyheter/m/rss/',\n",
    "    'https://rss.aftonbladet.se/rss2/small/pages/sections/senastenytt/',\n",
    "    'https://feeds.expressen.se/nyheter/',\n",
    "    'http://www.svd.se/?service=rss',\n",
    "    'http://api.sr.se/api/rss/program/83?format=145',\n",
    "    'http://www.svt.se/nyheter/rss.xml'\n",
    "]\n",
    "\n",
    "def fetch_and_parse_feeds():\n",
    "    \"\"\"Fetch and parse RSS feeds\"\"\"\n",
    "    posts = []\n",
    "\n",
    "    for url in RSS_FEED_URL:\n",
    "        try:\n",
    "            feed = feedparser.parse(url)\n",
    "            for entry in feed.entries:\n",
    "                published_raw = entry.get('published', '')\n",
    "                # Remove the trailing comma if published_raw is a tuple\n",
    "                if isinstance(published_raw, tuple):\n",
    "                    published_raw = published_raw[0]\n",
    "                \n",
    "                published_formatted = \"\"\n",
    "                # Try the email.utils parser first (handles RFC 2822 format with timezone)\n",
    "                try:\n",
    "                    parsed_time_tuple = email.utils.parsedate_tz(published_raw)\n",
    "                    if parsed_time_tuple:\n",
    "                        # Convert time tuple to UTC timestamp\n",
    "                        timestamp = email.utils.mktime_tz(parsed_time_tuple)\n",
    "                        # Convert timestamp to datetime object\n",
    "                        dt = datetime.fromtimestamp(timestamp)\n",
    "                        published_formatted = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                except:\n",
    "                    # Fallback to previous method if email.utils parser fails\n",
    "                    possible_formats = [\n",
    "                        \"%a, %d %b %Y %H:%M:%S %z\",  # Format with timezone offset\n",
    "                        \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "                        \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "                        \"%Y-%m-%dT%H:%M:%S.%fZ\",\n",
    "                        \"%Y-%m-%d %H:%M:%S\"\n",
    "                    ]\n",
    "                    \n",
    "                    for fmt in possible_formats:\n",
    "                        try:\n",
    "                            parsed_date = datetime.strptime(published_raw, fmt)\n",
    "                            published_formatted = parsed_date.strftime(\n",
    "                                \"%Y-%m-%d %H:%M:%S\")\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "                post = {\n",
    "                    'title': entry.get('title', 'No title'),\n",
    "                    'link': entry.get('link', 'No link'),\n",
    "                    'summary': entry.get('summary', 'No summary'),\n",
    "                    \"published\": published_formatted\n",
    "                }\n",
    "                posts.append(post)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse feed from {url}: {e}\")\n",
    "    return posts\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feeds = fetch_and_parse_feeds()\n",
    "    # Print to verify\n",
    "    print(json.dumps(feeds, indent=2))\n",
    "    print(len(feeds))\n",
    "\n",
    "# Expose feeds for import\n",
    "__all__ = ['feeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rss_feed_parser.py\n",
    "import feedparser\n",
    "import json\n",
    "from datetime import datetime\n",
    "import email.utils\n",
    "\n",
    "class RSSFeedParser:\n",
    "    def __init__(self, feed_urls):\n",
    "        self.feed_urls = feed_urls\n",
    "\n",
    "    def fetch_and_parse_feeds(self):\n",
    "        \"\"\"Fetch and parse RSS feeds\"\"\"\n",
    "        posts = []\n",
    "\n",
    "        for url in self.feed_urls:\n",
    "            try:\n",
    "                feed = feedparser.parse(url)\n",
    "                for entry in feed.entries:\n",
    "                    published_raw = entry.get('published', '')\n",
    "                    # Remove the trailing comma if published_raw is a tuple\n",
    "                    if isinstance(published_raw, tuple):\n",
    "                        published_raw = published_raw[0]\n",
    "                    \n",
    "                    published_formatted = \"\"\n",
    "                    # Try the email.utils parser first (handles RFC 2822 format with timezone)\n",
    "                    try:\n",
    "                        parsed_time_tuple = email.utils.parsedate_tz(published_raw)\n",
    "                        if parsed_time_tuple:\n",
    "                            # Convert time tuple to UTC timestamp\n",
    "                            timestamp = email.utils.mktime_tz(parsed_time_tuple)\n",
    "                            # Convert timestamp to datetime object\n",
    "                            dt = datetime.fromtimestamp(timestamp)\n",
    "                            published_formatted = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except:\n",
    "                        # Fallback to previous method if email.utils parser fails\n",
    "                        possible_formats = [\n",
    "                            \"%a, %d %b %Y %H:%M:%S %z\",  # Format with timezone offset\n",
    "                            \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "                            \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "                            \"%Y-%m-%dT%H:%M:%S.%fZ\",\n",
    "                            \"%Y-%m-%d %H:%M:%S\"\n",
    "                        ]\n",
    "                        \n",
    "                        for fmt in possible_formats:\n",
    "                            try:\n",
    "                                parsed_date = datetime.strptime(published_raw, fmt)\n",
    "                                published_formatted = parsed_date.strftime(\n",
    "                                    \"%Y-%m-%d %H:%M:%S\")\n",
    "                                break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "\n",
    "                    post = {\n",
    "                        'title': entry.get('title', 'No title'),\n",
    "                        'link': entry.get('link', 'No link'),\n",
    "                        'summary': entry.get('summary', 'No summary'),\n",
    "                        \"published\": published_formatted\n",
    "                    }\n",
    "                    posts.append(post)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse feed from {url}: {e}\")\n",
    "        return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rss_feed_saver.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class RSSFeedSaver:\n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser\n",
    "    \n",
    "    def save_feeds(self):\n",
    "        # Get feeds from the parser\n",
    "        feeds = self.parser.fetch_and_parse_feeds()\n",
    "        \n",
    "        # Generate filename with current date and time\n",
    "        current_time = datetime.now()\n",
    "        filename = f\"feeds_{current_time.strftime('%y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs('feeds', exist_ok=True)\n",
    "        filepath = os.path.join('feeds', filename)\n",
    "        \n",
    "        # Save feeds to JSON file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feeds, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(feeds)} feed entries to {filepath}\")\n",
    "        return filepath, len(feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 243 feed entries to feeds\\feeds_250216_160327.json\n",
      "Successfully saved 243 feed entries to feeds\\feeds_250216_160327.json\n"
     ]
    }
   ],
   "source": [
    "# main.py (example usage)\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your RSS feed URLs\n",
    "    RSS_FEED_URLS = [\n",
    "        'http://www.dn.se/nyheter/m/rss/',\n",
    "        'https://rss.aftonbladet.se/rss2/small/pages/sections/senastenytt/',\n",
    "        'https://feeds.expressen.se/nyheter/',\n",
    "        'http://www.svd.se/?service=rss',\n",
    "        'http://api.sr.se/api/rss/program/83?format=145',\n",
    "        'http://www.svt.se/nyheter/rss.xml'\n",
    "    ]\n",
    "    \n",
    "    # Create parser and saver\n",
    "    parser = RSSFeedParser(RSS_FEED_URLS)\n",
    "    saver = RSSFeedSaver(parser)\n",
    "    \n",
    "    # Save feeds\n",
    "    filepath, count = saver.save_feeds()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Successfully saved {count} feed entries to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, nltk_data_path='/c:/DIAD/ML/Tasks/T2/nltk_data'):\n",
    "        # Initialize NLTK\n",
    "        nltk.data.path.append(nltk_data_path)\n",
    "        \n",
    "        # Download resources only if needed\n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('swedish'))\n",
    "        self.stemmer = SnowballStemmer(\"swedish\")\n",
    "        self.categories = None\n",
    "        \n",
    "        # Suppress warnings\n",
    "        if not sys.warnoptions:\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Enhanced text cleaning\"\"\"\n",
    "        return (text\n",
    "                .str.lower()\n",
    "                .str.replace(r'http\\S+|www\\S+', '', regex=True)  # remove URLs\n",
    "                .str.replace(r'[^\\w\\s]', '', regex=True)         # remove punctuation - fixed with r prefix\n",
    "                .str.replace(r'\\d+', '', regex=True)             # remove digits - fixed with r prefix\n",
    "                .str.replace(r'<.*?>', '', regex=True)           # remove HTML tags\n",
    "                .str.replace(r'\\s+', ' ', regex=True)            # normalize whitespace\n",
    "                .str.strip())                                   # strip leading/trailing whitespace\n",
    "    \n",
    "    def remove_stop_words(self, sentence):\n",
    "        \"\"\"Remove stop words from text\"\"\"\n",
    "        return \" \".join([word for word in nltk.word_tokenize(sentence)\n",
    "                        if word not in self.stop_words])\n",
    "    \n",
    "    def stem_text(self, sentence):\n",
    "        \"\"\"Apply stemming to text\"\"\"\n",
    "        return \" \".join(self.stemmer.stem(word) for word in sentence.split())\n",
    "    \n",
    "    def prepare_data(self, data_path, apply_stemming=False):\n",
    "        \"\"\"Main data preparation pipeline\"\"\"\n",
    "        try:\n",
    "            # Load and shuffle data\n",
    "            data_raw = pd.read_csv(data_path).sample(frac=1, random_state=42)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Could not find data file at {data_path}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            raise ValueError(f\"The file at {data_path} is empty\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        if data_raw['Heading'].isnull().any():\n",
    "            print(f\"Warning: Found {data_raw['Heading'].isnull().sum()} null values in 'Heading' column\")\n",
    "            data_raw = data_raw.dropna(subset=['Heading'])\n",
    "        \n",
    "        # Get category columns\n",
    "        self.categories = list(data_raw.columns.values)[2:]\n",
    "        \n",
    "        # Clean and process text\n",
    "        data_raw['Heading'] = self.clean_text(data_raw['Heading'])\n",
    "        data_raw['Heading'] = data_raw['Heading'].apply(self.remove_stop_words)\n",
    "        \n",
    "        if apply_stemming:\n",
    "            data_raw['Heading'] = data_raw['Heading'].apply(self.stem_text)\n",
    "        \n",
    "        return data_raw\n",
    "    \n",
    "    def create_train_test_split(self, data, test_size=0.20):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        # Validation\n",
    "        if 'Heading' not in data.columns:\n",
    "            raise ValueError(\"Data must contain 'Heading' column\")\n",
    "        \n",
    "        if len(data) < 10:  # arbitrary minimum\n",
    "            raise ValueError(f\"Not enough data: {len(data)} rows\")\n",
    "        \n",
    "        # Split data\n",
    "        train, test = train_test_split(\n",
    "            data, random_state=42, test_size=test_size, shuffle=True)\n",
    "        \n",
    "        # Create TF-IDF features\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        \n",
    "        # Fit and transform training data\n",
    "        x_train = vectorizer.fit_transform(train['Heading'])\n",
    "        y_train = train.drop(labels=['Id', 'Heading'], axis=1)\n",
    "        \n",
    "        # Transform test data\n",
    "        x_test = vectorizer.transform(test['Heading'])\n",
    "        y_test = test.drop(labels=['Id', 'Heading'], axis=1)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "\n",
    "class TextClassifier:\n",
    "   def __init__(self):\n",
    "       self.models = self._initialize_models()\n",
    "       self.results = {}\n",
    "       self.best_models = {}\n",
    "       self._setup_logging()\n",
    "\n",
    "   def _setup_logging(self):\n",
    "       # Create directory if it doesn't exist\n",
    "       os.makedirs('logs', exist_ok=True)\n",
    "       filepath = os.path.join('logs', 'mltraining.log')\n",
    "       logging.basicConfig(filename=filepath, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "   def _initialize_models(self):\n",
    "       return {\n",
    "           \"Logistic Regression\": self._create_model_info(LogisticRegression(max_iter=1000), {\n",
    "               \"C\": [0.1, 1.0, 10.0],\n",
    "               \"penalty\": [\"l1\", \"l2\"],\n",
    "               \"solver\": [\"liblinear\"],\n",
    "           }),\n",
    "           \"Linear SVC\": self._create_model_info(LinearSVC(max_iter=1000), {\n",
    "               \"C\": [0.1, 1.0, 10.0],\n",
    "               \"loss\": [\"squared_hinge\"],\n",
    "               \"penalty\": [\"l1\", \"l2\"],\n",
    "               \"dual\": [False]\n",
    "           }),\n",
    "           \"SVM\": self._create_model_info(SVC(probability=True), {\n",
    "               \"C\": [0.1, 1.0, 10.0],\n",
    "               \"kernel\": [\"linear\", \"rbf\"],\n",
    "               \"class_weight\": [None, \"balanced\"]\n",
    "           }),\n",
    "           \"Random Forest\": self._create_model_info(RandomForestClassifier(random_state=42), {\n",
    "               \"n_estimators\": [100, 200],\n",
    "               \"max_depth\": [None, 10, 20],\n",
    "               \"min_samples_split\": [2, 5],\n",
    "               \"class_weight\": [None, \"balanced\"]\n",
    "           }),\n",
    "           \"Naive Bayes\": self._create_model_info(MultinomialNB(), {\n",
    "               \"alpha\": [0.1, 0.5, 1.0],\n",
    "               \"fit_prior\": [True, False]\n",
    "           })\n",
    "       }\n",
    "\n",
    "   def _create_model_info(self, estimator, param_grid):\n",
    "       return {\n",
    "           \"classifier\": OneVsRestClassifier(estimator) if estimator != RandomForestClassifier else MultiOutputClassifier(estimator),\n",
    "           \"param_grid\": {f\"estimator__{k}\": v for k, v in param_grid.items()}\n",
    "       }\n",
    "\n",
    "   def train_and_evaluate(self, x_train, y_train, x_test, y_test):\n",
    "       logging.info(\"Training and evaluating models...\")\n",
    "       for model_name, model_info in self.models.items():\n",
    "           self._train_model(model_name, model_info, x_train, y_train, x_test, y_test)\n",
    "\n",
    "   def _train_model(self, model_name, model_info, x_train, y_train, x_test, y_test):\n",
    "       logging.info(f\"\\n=== {model_name} ===\")\n",
    "       grid = GridSearchCV(model_info[\"classifier\"], model_info[\"param_grid\"], cv=5, scoring=\"accuracy\", n_jobs=-1, return_train_score=True)\n",
    "       grid.fit(x_train, y_train)\n",
    "\n",
    "       self.best_models[model_name] = grid.best_estimator_\n",
    "       y_pred = grid.predict(x_test)\n",
    "       self.results[model_name] = self._calculate_metrics(y_test, y_pred)\n",
    "\n",
    "       self._log_grid_results(grid)\n",
    "       self._log_results(model_name, y_test, y_pred)\n",
    "\n",
    "   def _calculate_metrics(self, y_test, y_pred):\n",
    "       return {\n",
    "           'accuracy': accuracy_score(y_test, y_pred),\n",
    "           'precision_micro': precision_score(y_test, y_pred, average='micro'),\n",
    "           'precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
    "           'recall_micro': recall_score(y_test, y_pred, average='micro'),\n",
    "           'recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
    "           'f1_micro': f1_score(y_test, y_pred, average='micro'),\n",
    "           'f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "       }\n",
    "\n",
    "   def _log_grid_results(self, grid):\n",
    "       logging.info(\"\\nGrid Search Results:\")\n",
    "       for i in range(len(grid.cv_results_['params'])):\n",
    "           logging.info(f\"Parameters: {grid.cv_results_['params'][i]}\")\n",
    "           logging.info(f\"Mean Test Score: {grid.cv_results_['mean_test_score'][i]}\")\n",
    "           logging.info(f\"Rank: {grid.cv_results_['rank_test_score'][i]}\")\n",
    "\n",
    "   def _log_results(self, model_name, y_test, y_pred):\n",
    "       metrics = self.results[model_name]\n",
    "       logging.info(f\"Results for {model_name}: {metrics}\")\n",
    "\n",
    "   def get_results_summary(self):\n",
    "       df = pd.DataFrame.from_dict(self.results, orient='index')\n",
    "       return df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "   def get_best_model(self):\n",
    "       results_df = self.get_results_summary()\n",
    "       best_model_name = results_df.index[0]\n",
    "       return best_model_name, self.best_models[best_model_name]\n",
    "\n",
    "def main():\n",
    "   preprocessor = TextPreprocessor()\n",
    "   data_path = \"./Book1.csv\"\n",
    "   processed_data = preprocessor.prepare_data(data_path, apply_stemming=False)\n",
    "   x_train, y_train, x_test, y_test, vectorizer = preprocessor.create_train_test_split(processed_data)\n",
    "\n",
    "   classifier = TextClassifier()\n",
    "   classifier.train_and_evaluate(x_train, y_train, x_test, y_test)\n",
    "\n",
    "   logging.info(\"\\nFinal Model Comparison:\")\n",
    "   logging.info(classifier.get_results_summary())\n",
    "\n",
    "   best_model_name, best_model = classifier.get_best_model()\n",
    "   logging.info(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
